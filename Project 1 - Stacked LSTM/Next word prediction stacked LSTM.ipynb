{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next word prediction \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical explanation of our goal\n",
    "\n",
    "Our goal is to, given an input of sequence of words, predict what the most likely next word is. \n",
    "\n",
    "In our case, a single word will be predicted, although predict more words is certainly possible. Single word is useful since it is more close to the autocomplete feature and accuracy is higher given a smaller dataset. Since we want to generate sequences longer than just 1 word, we will predict the next word and then use that predicted next word and append it to the previous input sequence. Using that newly updated input sequence we will be able to generate the next word in the sequence. This process can be repeated to generate as many sequence as we would like. \n",
    "\n",
    "To demonstrate, let's say that the number of features/words we predict on is 3 and our initial input sequence is [\"today\", \"was\", \"a\"] and our goal is to predict the next 2 words. Let us assume that our model predicted the next word in the sequence to be \"good\" giving us the total sequence to be [\"today\", \"was\", \"a\", \"good\"]. We can now move the sliding window of size 3 to use the last 3 words of this (we added \"good\" to the end of the input sequence and dropped \"today\"). We now use this new input sequence [\"was\", \"a\", \"good\"] to predic the next word which would ideally be [\"day\"] fulfilling Ice Cube's lyrics. However, it is entirelly possible that some other word could be predicted such as [\"boy\"]. This is entirely dependent on the distribution learned from the training data. \"was a good boy\" makes sense, however, \"today was a good boy\" does not, which is why we will use much larger sequences of words (around 50) to predict the next word. \n",
    "\n",
    "Essentially, the goal of the model is to learn and mimic from the training data as best as possible the probability distribution of the next word, given the previous words in the sequence - this is done through Bayes' theorem\n",
    "\n",
    "We can represent this as:\n",
    "\n",
    "$P(w_{t+1} | w_1, w_2, ..., w_{t})$\n",
    "\n",
    "where $w_{t+1}$ is the target word we want to predict, and $w_1$, $w_2$, ..., $w_{t}$ are the previous words in the sequence. $w_{t+1}$ can be any of the words in the vocabulary $V$. Vocabulary is the set of all possible words that can be predicted.\n",
    "\n",
    "Our goal is to find the word $w_{t+1}$ that maximizes the probability of the next word given the previous words. To do this, we find need to find the probability distribution for each word in the vocabulary.\n",
    "\n",
    "We can express this probability through the chain rule of probability by multiplying the probabilities of each word in the sequence:\n",
    "* Probability of the first word in the sequence being $w_1$ = $P(w_1)$\n",
    "* Probability of the second word in the sequence being $w_2$ given that the first word in the sequence is $w_1$ = $P(w_2 | w_1)$\n",
    "* Probability of the third word in the sequence being $w_3$ given that the first two words in the sequence are $w_1$ and $w_2$ = $P(w_3 | w_1, w_2)$\n",
    "\n",
    "In general, we have the probability that the word $w_i$ is word i, given that first i-1 words in the sequence are $w_1, w_2, ..., w_{i-1}$\n",
    "\n",
    "\n",
    "$ P(w(1), w(2), ..., w(i)) = P(w(1)) \\times P(w(2) | w(1)) \\times P(w(3) | w(1), w(2)) \\times ... \\times P(w(i) | w(1), w(2), ..., w(i-1)) = \\Pi_{t=1}^{T} P(w_{t} | w_{t},...,w_{1})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Below is the general overview of the project pipeline. Individual choices and decisions are explained in the corresponding sections.\n",
    "\n",
    "1. Get data\n",
    "    1. Keylogs\n",
    "        1. Use keylogger to record keystrokes\n",
    "        2. Merge keylogs into one file\n",
    "    2. Books and articles from Project Gutenberg\n",
    "        1. Download books and articles\n",
    "        2. Merge books and articles into one file\n",
    "<br><br>\n",
    "2. Data processing\n",
    "    1. Lowercase\n",
    "    2. Single whitespace\n",
    "    3. Remove punctuation\n",
    "    4. Remove numbers\n",
    "    5. Widen contractions\n",
    "    6. (keylogs) Fix typos\n",
    "    7. (keylogs) Remove emojis\n",
    "    8. [optional] (keylogs) Remove non-English\n",
    "    9. Remove stopwords\n",
    "    9. Split into tokens\n",
    "    10. Lemmatize or Stem (Lemmatize preferred)\n",
    "<br><br>\n",
    "3. Tokenization\n",
    "<br><br>\n",
    "4. Create features and target sequences (N-grams)\n",
    "<br><br>\n",
    "5. Split into train, validation, and test sets\n",
    "    1. Train: 80%\n",
    "    2. Validation: 10%\n",
    "    3. Test: 10%\n",
    "<br><br>\n",
    "6. Vectorization - Creating an embedding matrix from GloVe\n",
    "<br><br>\n",
    "7. Creating an LSTM sequential model\n",
    "<br><br>\n",
    "8. Evaluation\n",
    "    1. Perplexity\n",
    "    2. Word similarity\n",
    "<br><br>\n",
    "9. Prediction - Generating sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary installs:\n",
    "1. pip instal TextBlob\n",
    "2. pip install tensorflow (or tensorflow-gpu) and keras\n",
    "3. pip install nltk\n",
    "4. pip install git+https://github.com/MCFreddie777/language-check.git\n",
    "5. pip install contractions\n",
    "6. pip install pycontractions\n",
    "7. pip install numpy\n",
    "8. pip install scikit-learn\n",
    "9. pip install pandas\n",
    "10. pip install matplotlib\n",
    "11. pip install regex\n",
    "12. pip install pynput\n",
    "13. pip install win32gui\n",
    "14. pip install fuzzywuzzy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging text files / keylogger logs\n",
    "\n",
    "Given that the keylogger has been running for a few days or that there are several books, the txt files have to be merged into a single txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_txt_files(input_dir=\".\\keylogger\\logs\", output_dir=\"database/merged\", name=\"master.txt\"):\n",
    "    \"\"\"Merges all .txt files in the input directory into a single .txt file in the output directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        The path to the directory containing the .txt files to merge. Defaults to \".\\keylogger\\logs\".\n",
    "    output_dir : str\n",
    "        The path to the directory where the merged 'master.txt' file will be saved. Defaults to \"database\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None    \n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Create the output directory if it does not exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\n",
    "            \"Creating a folder '{output_dir}' to store the merged text file...\")\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Merge the contents of all .txt files in the input directory into a single string\n",
    "    merged_text = \"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_dir, filename), \"r\") as f:\n",
    "                merged_text += f.read()\n",
    "    print(\n",
    "        f\"Merged all .txt files from the {input_dir} folder into a single variable.\")\n",
    "\n",
    "    # Write the merged text to a new file in the output directory\n",
    "    output_filename = os.path.join(output_dir, name)\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        f.write(merged_text)\n",
    "    print(f\"Saved the merged text to ./{output_filename}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the master text file\n",
    "\n",
    "Below is the function used to read a text file and return a string containing the entire dataset which we use in our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_name, folder_path=\"./database/processed/\"):\n",
    "    \"\"\"Reads a text file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the text file.\n",
    "    folder_path : str (optional)\n",
    "        The path to the folder containing the text file. Defaults to \"./database/processed/\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        The text read from the file.\n",
    "    \"\"\"\n",
    "\n",
    "    import os  # Import the os module to work with file paths\n",
    "\n",
    "    text = open(os.path.join(folder_path, file_name), 'r').read()\n",
    "    print(\n",
    "        f\"\\nRead {file_name}. It contains {len(text)} characters and {len(text.split())} words.\")\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing is the fundamental process of preparing the data for the model. It is a very important step as it can have a significant impact on the model's performance (can reduce complexity and number of features, increase performance by finidng only the key features, etc.)\n",
    "\n",
    "Two libraries will be primarily used for preprocessing: regex and nltk. Regex is used for simpler text cleaning such as finding and removing punctuation, numbers, etc. NLTK is a very standard library used for more complex tasks such as tokenization, lemmatization, stemming, which can be easily done with its built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # regular expressions\n",
    "import nltk  # natural language toolkit\n",
    "# nltk.download() # uncomment to download the nltk data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase and stripping multiple whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_strip_whitespaces(text):\n",
    "    \"\"\"Converts a given text to lowercase and strips multiple whitespaces to a single whitespace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to convert.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The converted text with multiple whitespaces stripped to a single whitespace and converted to lowercase.\n",
    "    \"\"\"\n",
    "    # strip multiple whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_numbers(text):\n",
    "    \"\"\"Removes punctuation and numbers from a given text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to remove punctuation and numbers from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with punctuation and numbers removed.\n",
    "    \"\"\"\n",
    "\n",
    "    text = ''.join([char for char in text if char.isalpha() or char == ' '])\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Widen contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, model='contractions'):\n",
    "    \"\"\" \n",
    "    Expands contractions in a given text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to expand contractions in.\n",
    "    model : str\n",
    "        The model to use to expand contractions. Defaults to 'contractions'.\n",
    "        Options:\n",
    "            'contractions' - uses the contractions library\n",
    "            'pycontractions' - uses the pycontractions library. Greater accuracy as it looks at the context of the word. Has frequent dependency issues.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with contractions expanded.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if model == 'contractions':\n",
    "            import contractions\n",
    "            text = contractions.fix(text)\n",
    "            return text\n",
    "\n",
    "        elif model == 'pycontractions':\n",
    "            import pycontractions\n",
    "\n",
    "            # Load the contraction model\n",
    "            cont = pycontractions.Contractions(api_key=\"glove-twitter-100\")\n",
    "            cont.load_models()\n",
    "\n",
    "            # Expand contractions in the text\n",
    "            expanded_text = list(cont.expand_texts([text], precise=True))[0]\n",
    "            return expanded_text\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Model '{model}' is not supported. Please choose either 'contractions' or 'pycontractions'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error expanding contractions: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_stopwords(tokens):\n",
    "    \"\"\"Removes stopwords from a given list of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list\n",
    "        A list of tokens to remove stopwords from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filtered_words_tokens: list\n",
    "        A list of tokens with stopwords removed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    from string import punctuation\n",
    "    # since our text doesn't have punctuation we also remove punctuation from stop words\n",
    "    stopwords = [word for word in stopwords if word not in punctuation]\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    filtered_words = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "    return filtered_words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Removes emojis from a given text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to remove emojis from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        The text with emojis removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove emojis from the text\n",
    "    emoji = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "    text = emoji.sub(r'', text)\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes a given text into a list of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to tokenize. E.g. \"This is a sentence\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of words. E.g. [\"This\", \"is\", \"a\", \"sentence\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # split text into tokens (words) - also gets rid of multiple whitespaces\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct typos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_typos_textblob(text):\n",
    "    \"\"\"Corrects typoes in a given text using the TextBlob library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to correct typos in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with typos corrected.\n",
    "    \"\"\"\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = blob.correct()\n",
    "    text = corrected_text.string\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fuzzy matching - removing typos and non-english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_typos_fuzzy_match(text):\n",
    "    \"\"\"Fuzzy matches a given text to a corpus of words. Returns the closest match if the match is above a certain threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to fuzzy match.\n",
    "    corpus : list\n",
    "        A list of words to use as a reference for fuzzy matching the text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The fuzzy matched text.\n",
    "    \"\"\"\n",
    "    from fuzzywuzzy import process, fuzz\n",
    "    from nltk.corpus import words\n",
    "\n",
    "    corpus = words.words()\n",
    "\n",
    "    # Split the text into a list of words\n",
    "    words = text.split()\n",
    "\n",
    "    # Correct each word in the text\n",
    "    fuzzy_matched_words = []\n",
    "    for word in words:\n",
    "        # Find the closest match to the word in the corpus\n",
    "        closest_match = process.extractOne(word, corpus, scorer=fuzz.ratio)\n",
    "        # If the closest match is a perfect match, use it\n",
    "        if closest_match[1] > 85:\n",
    "            fuzzy_matched_words.append(closest_match[0])\n",
    "\n",
    "    # Join the corrected words into a string\n",
    "    fuzzy_matched_text = \" \".join(fuzzy_matched_words)\n",
    "    return fuzzy_matched_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokens):\n",
    "    \"\"\"Lemmatizes a list of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list\n",
    "        A list of words to lemmatize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized_words : list\n",
    "        A list of lemmatized words.\n",
    "    \"\"\"\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(text, save_file=False, directory='./database/processed/', verbose=False, remove_stopwords=True, lemmatize=True,  to_remove_emojis=False, to_correct_typos=False, expand_contractions_model='contractions'):\n",
    "    \"\"\"\n",
    "    Preprocesses the text data by: converting to lowercase, removing punctuation, \n",
    "                                      splitting into tokens, removing stop words, and lemmatizing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to preprocess.\n",
    "    remove_stopwords : bool, optional\n",
    "        Whether to remove stopwords from the text, by default True\n",
    "    lemmatize : bool, optional\n",
    "        Whether to lemmatize the text, by default True\n",
    "    remove_emojis : bool, optional\n",
    "        Whether to remove emojis from the text, by default False\n",
    "    correct_typos : var, optional\n",
    "        Whether to correct typos in the text, by default False. Extremelly slow, use only if absolutely necessary.\n",
    "        You can pass 'textblob' or 'fuzzy_match' to correct typos using the TextBlob library or fuzzy matching, respectively.\n",
    "    expand_contractions_model : var, optional\n",
    "        Model used to expand contractions, by default 'contractions'. \n",
    "        You can pass 'contractions' or 'pycontractions' to expand contractions using the contractions library or nltk, respectively.\n",
    "    save_file : string or False, optional\n",
    "        Variable to save the preprocessed text to a file, by default False. \n",
    "        If you want to save the file, pass the file name as a string.\n",
    "        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('\\nPreprocessing text...')\n",
    "\n",
    "    # convert to lowercase and strip multiple whitespaces\n",
    "    text = lowercase_and_strip_whitespaces(text)\n",
    "    if verbose:\n",
    "        print('\\tConverted to lowercase and stripped multiple whitespaces.')\n",
    "\n",
    "    # remove punctuation and numbers\n",
    "    text = remove_punctuation_and_numbers(text)\n",
    "    if verbose:\n",
    "        print('\\tRemoved punctuation and numbers.')\n",
    "\n",
    "    # remove emojis\n",
    "    if to_remove_emojis:\n",
    "        text = remove_emojis(text)\n",
    "        if verbose:\n",
    "            print('\\tRemoved emojis.')\n",
    "\n",
    "    # correct typos\n",
    "    if to_correct_typos is not False:\n",
    "        if verbose:\n",
    "            print('\\tCorrecting typos... This may take a while...')\n",
    "        if to_correct_typos == 'textblob':\n",
    "            text = correct_typos_textblob(text)\n",
    "        elif to_correct_typos == 'fuzzy_match':\n",
    "            text = correct_typos_fuzzy_match(text)\n",
    "        if verbose:\n",
    "            print(f'\\tTypos corrected using {to_correct_typos}.')\n",
    "\n",
    "    # expand contractions\n",
    "    if verbose:\n",
    "        print(f'\\tExpanding contractions using {expand_contractions_model} model...')\n",
    "    text = expand_contractions(text, model=expand_contractions_model)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenize(text)\n",
    "    if verbose:\n",
    "        print('\\tSplit the text into tokens.')\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = eliminate_stopwords(tokens)\n",
    "        if verbose:\n",
    "            print('\\tRemoved stopwords.')\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens = lemmatize_words(tokens)\n",
    "        if verbose:\n",
    "            print('\\tLemmatized words.')\n",
    "\n",
    "    # save preprocessed text to file\n",
    "    if save_file is not False:\n",
    "        with open(directory + save_file, 'w') as f:\n",
    "            f.write(' '.join(tokens))\n",
    "            if verbose:\n",
    "                print(f'\\tPreprocessed text saved to {directory + save_file}.')\n",
    "    if verbose:\n",
    "        print(f'Preprocessing finished. There are now {len(tokens)} tokens.\\n')\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization: word to index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_to_index_mappings(tokens):\n",
    "    \"\"\"Creates word-to-index and index-to-word mappings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list\n",
    "        A list of tokens. E.g. ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word_index : dict\n",
    "        A dictionary with word-to-index mappings. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
    "    index_word : dict\n",
    "        A dictionary with index-to-word mappings. E.g. {1: 'the', 2: 'cat', 3: 'sat', 4: 'on', 5: 'mat'}\n",
    "    text_as_tokens : list\n",
    "        A list of tokens. [1, 2, 3, 4, 1, 5]\n",
    "    \"\"\"\n",
    "\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from pickle import dump\n",
    "\n",
    "    # create a tokenizer object\n",
    "    # lower=True converts all text to lowercase, oov_token='<OOV>' replaces all out-of-vocabulary words with <OOV>\n",
    "    tokenizer = Tokenizer(lower=True, oov_token='<OOV>')\n",
    "\n",
    "    # fit the tokenizer on the text data\n",
    "    tokenizer.fit_on_texts(tokens)\n",
    "\n",
    "    # get the word-to-index mappings\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\n",
    "        f'Created word-to-index dictionary. Total number of unique tokens: {len(word_index)}.')\n",
    "\n",
    "    # get the index-to-word mappings\n",
    "    index_word = {v: k for k, v in word_index.items()}\n",
    "\n",
    "    # convert the text to a list of tokens\n",
    "    # the output is a list of lists, so we take the first element\n",
    "    text_tokenized = tokenizer.texts_to_sequences([tokens])[0]\n",
    "\n",
    "    # save the tokenizer object\n",
    "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "    return word_index, index_word, text_tokenized\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into features and targets: N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_targets(text_tokenized, total_unique_tokens, seq_len=30):\n",
    "    \"\"\"Creates features and targets from a list of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_tokenized : list\n",
    "        The list of tokens - the entire vocabulary tokenized. E.g. [1, 2, 3, 4, 1, 5]\n",
    "    total_unique_tokens : int\n",
    "        The total number of unique tokens in the vocabulary. E.g. 5\n",
    "    seq_len : int, optional\n",
    "        The length of the sequences, by default 5. \n",
    "        If seq_len=5, the testing sequence will be made of 4 tokens and the target will be the 5th token.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : matrix\n",
    "        A list of sequences. # E.g. [[1, 2, 3, 4], [2, 3, 4, 1], [3, 4, 1, 5]]\n",
    "    targets : matrix\n",
    "        A list of targets. E.g. [[0,0,0,1,0], [0,0,0,0,1], [0,1,0,0,0]] \n",
    "        One-hot encoded. 1 only for the target word in the vocabulary, everything else 0\n",
    "    \"\"\"\n",
    "    from keras.utils import to_categorical  # one-hot encoding\n",
    "\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(seq_len, len(text_tokenized)):\n",
    "        seq = text_tokenized[i-seq_len:i]\n",
    "        target = text_tokenized[i]\n",
    "        features.append(seq)\n",
    "        target_one_hot = to_categorical(\n",
    "            target, num_classes=total_unique_tokens)\n",
    "        targets.append(target_one_hot)\n",
    "\n",
    "    if len(features) != len(targets):\n",
    "        raise ValueError(\n",
    "            f'Number of feature examples ({len(features)}) is different from number of targets ({len(targets)}).')\n",
    "\n",
    "    print(\n",
    "        f'Created feature ({len(features[0])} words) and target (1 word) pairs. Total number of datapoints: {len(features)}.')\n",
    "    return features, targets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training testing and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_testing_validation(features, targets, test_size=0.1, val_size=0.1):\n",
    "    \"\"\" \n",
    "    Splits the text into training, testing, and validation sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to split.\n",
    "    test_size : float, optional\n",
    "        The size of the testing set, by default 0.1\n",
    "    val_size : float, optional\n",
    "        The size of the validation set, by default 0.1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : list\n",
    "        A list of training features.\n",
    "    X_test : list\n",
    "        A list of testing features.\n",
    "    X_val : list\n",
    "        A list of validation features.\n",
    "    y_train : list\n",
    "        A list of training targets.\n",
    "    y_test : list\n",
    "        A list of testing targets.\n",
    "    y_val : list\n",
    "        A list of validation targets.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, targets, test_size=test_size+val_size, random_state=56, shuffle=True)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_test, y_test, test_size=val_size/(test_size+val_size), random_state=56, shuffle=True) \n",
    "   \n",
    "\n",
    "    print(\n",
    "        f\"Split dataset into training ({(1-test_size-val_size)*100}%), validation ({val_size*100}%), testing({test_size*100}%). Sizes: X_train: {len(X_train)}, X_test: {len(X_test)}, X_val: {len(X_val)}\")\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index, glove_dir='./glove/glove.6B', embedding_dim=100):\n",
    "    \"\"\"Loads the GloVe embedding matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_index : dict\n",
    "        A dictionary with the word index. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
    "    glove_dir : str, optional\n",
    "        The directory where the GloVe embeddings are stored, by default './glove/glove.6B'. \n",
    "        Can be downloaded from https://nlp.stanford.edu/projects/glove/. \n",
    "    embedding_dim : int, optional\n",
    "        The dimension of the GloVe embeddings, by default 100\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embedding_matrix : np.array\n",
    "        A numpy array with the embedding matrix. Dimensions: (num_words, embedding_dim) \n",
    "        Used to initialize the embedding layer in the model. ith row corresponds to the ith vector in the word_index.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    try:\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join(\n",
    "            glove_dir, f'glove.6B.{embedding_dim}d.txt'), encoding='utf-8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print(f'\\nLoaded glove. Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "        # prepare embedding matrix\n",
    "        num_words = len(word_index) + 1\n",
    "        embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print(\n",
    "            f'Created embedding matrix. Dimensions: {embedding_matrix.shape}.')\n",
    "        return embedding_matrix\n",
    "    except Exception as e:\n",
    "        print('Error occured:', e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining custom metric - perplexity\n",
    "def perplexity(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Calculates the perplexity of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tensor\n",
    "        The true targets.\n",
    "    y_pred : tensor\n",
    "        The predicted targets.\n",
    "    \"\"\"\n",
    "    import keras.backend as K\n",
    "\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.exp(cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(word_index, embedding_matrix, X_train, X_test, X_val, y_train, y_test, y_val, epochs=100, batch_size=256, lr=0.001, embedding_dim=100, seq_len=30, dropout_rate=0.2, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_index : dict\n",
    "        A dictionary with the word index. E.g. {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
    "    embedding_matrix : np.array\n",
    "        A numpy array with the embedding matrix. Dimensions: (num_words, embedding_dim)\n",
    "    epochs : int, optional\n",
    "        The number of epochs to train the model, by default 100\n",
    "    X_train : list\n",
    "        A list of training features.\n",
    "    X_test : list\n",
    "        A list of testing features.\n",
    "    X_val : list\n",
    "        A list of validation features.\n",
    "    y_train : list\n",
    "        A list of training targets.\n",
    "    y_test : list\n",
    "        A list of testing targets.\n",
    "    y_val : list\n",
    "        A list of validation targets.\n",
    "    batch_size : int, optional\n",
    "        The batch size, by default 256\n",
    "    lr : float, optional\n",
    "        The learning rate, by default 0.001\n",
    "    embedding_dim : int, optional\n",
    "        The dimension of the GloVe embeddings, by default 100\n",
    "    seq_len : int, optional\n",
    "        The length of the sequences, by default 30\n",
    "    dropout_rate : float, optional\n",
    "        The dropout rate, by default 0.2\n",
    "    weight_decay : float, optional\n",
    "        The weight decay, by default 1e-3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras model\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from keras import regularizers\n",
    "    from pickle import dump\n",
    "    from keras.metrics import categorical_accuracy, CosineSimilarity\n",
    "\n",
    "    num_words = len(word_index) + 1  # +1 because of the 0 index\n",
    "\n",
    "    print(\"\\nStarting LSTM model training...\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, weights=[\n",
    "        embedding_matrix], input_length=seq_len, trainable=False))\n",
    "    model.add(LSTM(units=50, return_sequences=True,\n",
    "              kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Dropout(dropout_rate)) # used to prevent overfitting\n",
    "    model.add(LSTM(50, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Dropout(dropout_rate)) # used to prevent overfitting\n",
    "    model.add(Dense(50, activation='relu',\n",
    "              kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # perplexity\n",
    "    def perplexity(y_true, y_pred):\n",
    "        \"\"\" \n",
    "        Calculates the perplexity of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : tensor\n",
    "            The true targets.\n",
    "        y_pred : tensor\n",
    "            The predicted targets.\n",
    "        \"\"\"\n",
    "        import keras.backend as K\n",
    "\n",
    "        cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "        perplexity = K.exp(cross_entropy)\n",
    "        return perplexity\n",
    "\n",
    "    # compile model\n",
    "    my_opt = Adam(learning_rate=lr)\n",
    "    # model.compile(loss='categorical_crossentropy',\n",
    "    #               optimizer=my_opt, \n",
    "    #               metrics=['accuracy', perplexity])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=my_opt, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = \"training_1\"\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                  save_weights_only=True,\n",
    "                                  verbose=0,\n",
    "                                  save_freq='epoch')\n",
    "    # patience is the number of epochs to wait before stopping, if the model is not improving.\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_accuracy', verbose=0, patience=3, restore_best_weights=True)\n",
    "\n",
    "    import numpy as np\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # fit model\n",
    "    from keras.utils import custom_object_scope\n",
    "    \"\"\" \n",
    "    We need to register the custom metric function using the keras.utils.custom_object_scope \n",
    "    so the 'perplexity' function becomes recognized as a valid metric function by Keras.\n",
    "    \"\"\"\n",
    "    # with custom_object_scope({'perplexity': perplexity}):\n",
    "    history = model.fit(X_train, y_train, validation_data=(\n",
    "        X_val, y_val), batch_size=batch_size, epochs=epochs, shuffle=True, callbacks=[earlystopping]) #, cp_callback])\n",
    "\n",
    "    # save the model to file\n",
    "    model_name = f\"saved_models/lstm_{embedding_dim}d_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch.h5\"\n",
    "    model.save(model_name)\n",
    "    history_name = f\"saved_models/lstm_{embedding_dim}d_{seq_len}seq_{epochs}epochs_{lr}lr_{batch_size}batch_history.pkl\"\n",
    "    dump(history.history, open(history_name, 'wb'))\n",
    "\n",
    "    # evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print('Test Loss: %f' % (loss))\n",
    "    print('Test Accuracy: %f' % (accuracy))\n",
    "    #print('Test Perplexity: %f' % (perplexity))\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras history\n",
    "        The history of the training.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # plot training and validation loss\n",
    "    plt.plot(history['loss'], label='train')\n",
    "    plt.plot(history['val_loss'], label='validation')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot training and validation loss\n",
    "    plt.plot(history['accuracy'], label='train')\n",
    "    plt.plot(history['val_accuracy'], label='validation')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot training and validation perplexity\n",
    "    # plt.plot(history['perplexity'], label='train')\n",
    "    # plt.plot(history['val_perplexity'], label='validation')\n",
    "    # plt.title('Training and validation perplexity')\n",
    "    # plt.ylabel('Perplexity')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    \"\"\"\n",
    "    Generates a sequence of n_words using the trained model from the seed_text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras model\n",
    "        The trained model.\n",
    "    tokenizer : keras tokenizer\n",
    "        The tokenizer used to tokenize the text.\n",
    "    seq_length : int\n",
    "        The length of the sequences.\n",
    "    seed_text : str\n",
    "        The seed text.\n",
    "    n_words : int\n",
    "        The number of words to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    from keras.utils import pad_sequences\n",
    "    import numpy as np\n",
    "    result = list()\n",
    "    in_text = seed_text  \n",
    "    # generate a fixed number of words\n",
    "    in_text = data_preprocessing(in_text, save_file=False, verbose=False)\n",
    "    # remove words that are not in the vocabulary\n",
    "    in_text = [word for word in in_text if word in tokenizer.word_index]\n",
    "    in_text = \" \".join(in_text)\n",
    "\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0] # [0] because it returns a list of lists\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)  \n",
    "        yhat_max = np.argmax(yhat[0]) # get the index of the highest probability\n",
    "\n",
    "        # map predicted word index to word\n",
    "        predicted_word = ''\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat_max:\n",
    "                predicted_word = word\n",
    "        #append to input\n",
    "        in_text += ' ' + predicted_word\n",
    "        result.append(predicted_word)\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tying it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read Book1.txt. It contains 474402 characters and 83183 words.\n",
      "\n",
      "Preprocessing text...\n",
      "\tConverted to lowercase and stripped multiple whitespaces.\n",
      "\tRemoved punctuation and numbers.\n",
      "\tExpanding contractions using contractions model...\n",
      "\tSplit the text into tokens.\n",
      "\tLemmatized words.\n",
      "\tPreprocessed text saved to ./database/processed/Book1.txt_processed.\n",
      "Preprocessing finished. There are now 82488 tokens.\n",
      "\n",
      "Created word-to-index dictionary. Total number of unique tokens: 5363.\n",
      "Created feature (30 words) and target (1 word) pairs. Total number of datapoints: 82458.\n",
      "Split dataset into training (80.0%), validation (15.0%), testing(5.0%). Sizes: X_train: 65966, X_test: 4123, X_val: 12369\n",
      "\n",
      "Loaded glove. Found 400000 word vectors.\n",
      "Created embedding matrix. Dimensions: (5364, 50).\n",
      "\n",
      "SEQUENCE_LENGTH: 30, EPOCHS: 20, BATCH_SIZE: 128, EMBEDDING_DIM: 50, LR: 0.001\n",
      "\n",
      "Starting LSTM model training...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 30, 50)            268200    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 30, 50)            20200     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 30, 50)            0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5364)              273564    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 584,714\n",
      "Trainable params: 316,514\n",
      "Non-trainable params: 268,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "516/516 [==============================] - 150s 287ms/step - loss: 6.5749 - accuracy: 0.0461 - val_loss: 6.3974 - val_accuracy: 0.0498\n",
      "Epoch 2/20\n",
      "516/516 [==============================] - 150s 291ms/step - loss: 6.3282 - accuracy: 0.0481 - val_loss: 6.3503 - val_accuracy: 0.0523\n",
      "Epoch 3/20\n",
      "348/516 [===================>..........] - ETA: 45s - loss: 6.2203 - accuracy: 0.0510"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [599], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mError while plotting results: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     run_everything()\n",
      "Cell \u001b[1;32mIn [599], line 52\u001b[0m, in \u001b[0;36mrun_everything\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m embedding_matrix \u001b[39m=\u001b[39m load_glove(word_index, embedding_dim\u001b[39m=\u001b[39mEMBEDDING_DIM)\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     50\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSEQUENCE_LENGTH: \u001b[39m\u001b[39m{\u001b[39;00mSEQUENCE_LENGTH\u001b[39m}\u001b[39;00m\u001b[39m, EPOCHS: \u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, BATCH_SIZE: \u001b[39m\u001b[39m{\u001b[39;00mBATCH_SIZE\u001b[39m}\u001b[39;00m\u001b[39m, EMBEDDING_DIM: \u001b[39m\u001b[39m{\u001b[39;00mEMBEDDING_DIM\u001b[39m}\u001b[39;00m\u001b[39m, LR: \u001b[39m\u001b[39m{\u001b[39;00mLR\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m model, history \u001b[39m=\u001b[39m train_lstm(word_index, embedding_matrix, epochs\u001b[39m=\u001b[39;49mEPOCHS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, lr\u001b[39m=\u001b[39;49mLR, embedding_dim\u001b[39m=\u001b[39;49mEMBEDDING_DIM,\n\u001b[0;32m     53\u001b[0m                             seq_len\u001b[39m=\u001b[39;49mSEQUENCE_LENGTH, X_train\u001b[39m=\u001b[39;49mX_train, X_val\u001b[39m=\u001b[39;49mX_val, y_train\u001b[39m=\u001b[39;49my_train, y_val\u001b[39m=\u001b[39;49my_val, X_test\u001b[39m=\u001b[39;49mX_test, y_test\u001b[39m=\u001b[39;49my_test)\n\u001b[0;32m     56\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtokenizer.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     57\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlstm_\u001b[39m\u001b[39m{\u001b[39;00mEMBEDDING_DIM\u001b[39m}\u001b[39;00m\u001b[39md_\u001b[39m\u001b[39m{\u001b[39;00mSEQUENCE_LENGTH\u001b[39m}\u001b[39;00m\u001b[39mseq_\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39mepochs_\u001b[39m\u001b[39m{\u001b[39;00mLR\u001b[39m}\u001b[39;00m\u001b[39mlr_\u001b[39m\u001b[39m{\u001b[39;00mBATCH_SIZE\u001b[39m}\u001b[39;00m\u001b[39mbatch.h5\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn [596], line 122\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[1;34m(word_index, embedding_matrix, X_train, X_test, X_val, y_train, y_test, y_val, epochs, batch_size, lr, embedding_dim, seq_len, dropout_rate, weight_decay)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39mWe need to register the custom metric function using the keras.utils.custom_object_scope \u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mso the 'perplexity' function becomes recognized as a valid metric function by Keras.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m# with custom_object_scope({'perplexity': perplexity}):\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    123\u001b[0m     X_val, y_val), batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49m[earlystopping]) \u001b[39m#, cp_callback])\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m# save the model to file\u001b[39;00m\n\u001b[0;32m    126\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved_models/lstm_\u001b[39m\u001b[39m{\u001b[39;00membedding_dim\u001b[39m}\u001b[39;00m\u001b[39md_\u001b[39m\u001b[39m{\u001b[39;00mseq_len\u001b[39m}\u001b[39;00m\u001b[39mseq_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39mepochs_\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39mlr_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39mbatch.h5\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\risti\\miniconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_everything():\n",
    "    from pickle import load\n",
    "    from keras.models import load_model\n",
    "    import os\n",
    "\n",
    "    SEED_TEXT = \"today was such a sad day. i really wanted to go and meet harry. he helped me so much with my homework.\"\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 128\n",
    "    EMBEDDING_DIM = 50\n",
    "    LR = 0.001\n",
    "    NUM_WORDS_TO_GENERATE = 30\n",
    "\n",
    "    # # merge all the data\n",
    "    # merge_txt_files(input_dir='./database/hp_unprocessed',\n",
    "    #                 output_dir='database/merged/', name='hp_unprocessed_merged.txt')\n",
    "\n",
    "    FILE_NAME = 'Book1.txt'\n",
    "    # read the data\n",
    "    text = read_txt_file(file_name= FILE_NAME,\n",
    "                         folder_path='./database/hp_unprocessed/')\n",
    "\n",
    "    # preprocess the data\n",
    "    preprocessed_tokens = data_preprocessing(text, save_file=f'{FILE_NAME}_processed', directory='./database/processed/', verbose=True, remove_stopwords=False,\n",
    "                                             lemmatize=True,  to_remove_emojis=False, to_correct_typos=False)\n",
    "\n",
    "    del text\n",
    "\n",
    "    # # create the word to index mappings\n",
    "    word_index, index_word, text_tokenized = get_word_to_index_mappings(\n",
    "        preprocessed_tokens)\n",
    "\n",
    "    # # # get the features and targets\n",
    "    features, targets = get_features_targets(\n",
    "        text_tokenized, total_unique_tokens=len(word_index)+1, seq_len=SEQUENCE_LENGTH)\n",
    "    \n",
    "    #save features, targets in df file\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'features': features, 'targets': targets})\n",
    "    df.to_csv('features_targets.csv', index=False)\n",
    "    del df\n",
    "\n",
    "    # # # split the data into training, testing and validation sets\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = split_training_testing_validation(\n",
    "        features, targets, test_size=0.05, val_size=0.15)\n",
    "\n",
    "    embedding_matrix = load_glove(word_index, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "    print(\n",
    "        f\"\\nSEQUENCE_LENGTH: {SEQUENCE_LENGTH}, EPOCHS: {EPOCHS}, BATCH_SIZE: {BATCH_SIZE}, EMBEDDING_DIM: {EMBEDDING_DIM}, LR: {LR}\")\n",
    "    \n",
    "    model, history = train_lstm(word_index, embedding_matrix, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, embedding_dim=EMBEDDING_DIM,\n",
    "                                seq_len=SEQUENCE_LENGTH, X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val, X_test=X_test, y_test=y_test)\n",
    "    \n",
    "    \n",
    "    tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "    model_name = f'lstm_{EMBEDDING_DIM}d_{SEQUENCE_LENGTH}seq_{EPOCHS}epochs_{LR}lr_{BATCH_SIZE}batch.h5'\n",
    "    saved_model_dir = 'saved_models'\n",
    "    model_loaded = load_model(os.path.join(\n",
    "        saved_model_dir, model_name))\n",
    "    # model_loaded = load_model(os.path.join(\n",
    "    #     saved_model_dir, model_name), custom_objects={'perplexity': perplexity})\n",
    "    history_loaded = load(open(os.path.join(\n",
    "        saved_model_dir, f'lstm_{EMBEDDING_DIM}d_{SEQUENCE_LENGTH}seq_{EPOCHS}epochs_{LR}lr_{BATCH_SIZE}batch_history.pkl'), 'rb'))\n",
    "\n",
    "    generated_seq = generate_seq(\n",
    "        model_loaded, tokenizer, SEQUENCE_LENGTH, seed_text=SEED_TEXT, n_words=NUM_WORDS_TO_GENERATE)\n",
    "    print(f'\\nGenerating text...\\nInput: {SEED_TEXT}. \\nGenerated text: {generated_seq}\\n')\n",
    "\n",
    "    try:\n",
    "        plot_results(history_loaded)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError while plotting results: {e}\")\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_everything()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "924b6ded23d1260d56f01f9496c1fa7915f2e054489eccd3a656c41d58155093"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

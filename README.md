# Personalized Autocomplete: Next-word-prediction-task

This project is the result of my semester-long exploration into next word prediction tasks, where I aimed to create a personalized autocomplete system using three distinct architectures: stacked LSTMs, Seq2Seq with Attention and LSTMs, and GPT-2.

Divided into three large subprojects, each architecture is progressively more complex and powerful. The project starts with the stacked LSTMs, followed by Seq2Seq with Attention and LSTMs, and finally culminates with GPT-2, which I tried to write out completely from scratch. While it might be perhaps ideal to follow the order presented in this repository, starting from the simplest to the most advanced, I understand that it might be time-consuming and repetitive. Therefore, I recommend just focusing on the final GPT-2 project.

The GPT-2 project is not only the most intriguing and intricate but also includes detailed information about the previous architectures and their explanations. It serves as a comprehensive overview of the entire project.

If you have any questions or would like access to the Colab notebooks, feel free to reach out to me.